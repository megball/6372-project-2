---
title: "rinku_random_forest"
author: "Rinku Lichti"
date: "11/27/2020"
output: html_document
---

# Load the data

```{r}
train <- read.csv("../data/train.csv", stringsAsFactors = TRUE)
test <- read.csv("../data/test.csv", stringsAsFactors = TRUE)
```

# Train a Random Forest, tuning mtry and splitrule

```{r}
set.seed(1234)
cv_control <- trainControl(method="cv", 
                     classProbs = TRUE,
                     savePredictions = TRUE,
                     summaryFunction = twoClassSummary,
                     num = 5)

rf_grid <- expand.grid(
  mtry = 4:8,
  splitrule = c("gini","extratrees", "hellinger"),
  min.node.size = c(1)
)

fitRF <- train(y ~ ., 
               data = train, 
               method = "ranger", 
               metric = "ROC",
               trControl = cv_control,
               num.threads = 6,
               num.trees = 30,
               tuneGrid=rf_grid)  
fitRF
```

I chose to tune 2 hyper parameters for Random Forest
1. mtry which represents the number of predictors considered when splitting a node in a tree
2. splitrule which determines the rule used for the actual splitting based on the above predictors
Note: I set min.node.size to 1 as appropriate for classification

# Performance on Training Set

```{r}
plot(fitRF)
confusionMatrix(fitRF)
```
Optimizing for ROC, the winning parameters are an mtry of 5 predictors considered at each split, and the Hellinger split rule.  It's interesting that Hellinger won. I found some papers suggesting Hellinger handles imbalanced data well; being insensitive to skew.*
* CITATION: https://www3.nd.edu/~nchawla/papers/DMKD11.pdf
* CITATION: https://medium.com/@evgeni.dubov/classifying-imbalanced-data-using-hellinger-distance-f6a4330d6f9a

# Performance on Test Set

```{r}
fitRF.predictions.raw <- predict(fitRF, newdata = test, type="raw")
fitRF.predictions.prob <- predict(fitRF, newdata = test, type="prob")
confusionMatrix(fitRF.predictions.raw, test$y, positive="yes")
```

Using the default cutoff, our random forest gets a test accuracy of 0.9161, with Sensitivity 0.4953 and Specificity 0.9688.

# ROC Curve and Optimal Cutoff

```{r warning=F, message=F}
prediction.probabilities <- fitRF.predictions.prob$yes
predicted.classes <- fitRF.predictions.raw
observed.classes <- test$y

# Compute roc
res.roc <- roc(observed.classes, prediction.probabilities)
plot.roc(res.roc, print.auc = TRUE, print.thres = "best")

# If we wanted cutoffs for specific specificities...
#roc.data <- data_frame(
#  thresholds = res.roc$thresholds,
#  sensitivity = res.roc$sensitivities,
#  specificity = res.roc$specificities
#)
# Get the probability threshold for specificity = 0.6
#roc.data %>% filter(specificity >= 0.6)

```
The most balanced cutoff for this model is 0.117, which gives 


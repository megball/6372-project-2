---
title: "Project2"
author: "Rinku Lichti, Simerpreet Reddy, Megan Ball"
date: "11/4/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#load libraries
library(dplyr)
library(tidyverse)
library(ggplot2)
library(caret)
library(e1071)
library(class)
library(gridExtra)
library(summarytools)
library(gt)
library(corrplot)
library(GGally)
```

```{r Import data}
full <- read_delim(here::here("data", "bank-additional-full.csv"),';')
str(full)
head(full)

nrow(full) 
ncol(full)
```


```{r}
# Clean up column names
full <- janitor::clean_names(full)

summary(full)
print(dfSummary(full, graph.magnif = 0.75), method = 'browser')
str(full)

# Check for missing values
tibble(variable = names(colSums(is.na(full))),
       missing = colSums(is.na(full))) %>% 
  gt() %>% 
  tab_header(title = "Missing Values in Data") 


```

Looking at the dfsummary, there doesnt seem to be missing data in terms of just not having values. However, there are some fields that have explicit unknown or non-existent classes that could be considered as 'missing'. For example, loan and housing have 990 "unknown" values. And 'default' has 8597 "unknown" values representing 20.9% 

pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)

# Remove missing values
```{r}
#remove "unknowns" based on small sample sizes compared to full data set
df <- full %>%
  filter(loan != "unknown")

#down to 40,198 obs

df <- df %>%
  filter(marital != "unknown")

#down to 40,119 obs

df <- df %>%
  filter(education != "unknown")

#down to 38,437 obs

df <- df %>%  filter(job != "unknown")
nrow(df)

#recheck summary
summary(df)

```

Remove column default from the analysis. REasons for removing the columns: 
1) After cleaning up the data set of the other 'unknowns', this column has 7,757 unknown values as well. With only 3 values as 'yes',and 30,485 as 'no', it is difficult to impute values
2) Practically, column default would need to be used before the campaign, the sales person needs to decide if the person with a default needs to be approached to or not, not after the campaign, so it appears that it is ok to let go of this column when predicting the outcome of the campaign. 

```{r}
df <- df %>%  select(-c(default))
nrow(df)
str(df)
summary(df)
```


```{r}
#change some variables to factor
cols <- c("job", "marital", "education","housing","loan","contact","month","day_of_week","poutcome","y")

df[cols] <- lapply(df[cols], factor) 

str(df)

#make sure "success" level is defined as "yes"
str(df$y)
summary(df$y)

```

# Exploratory Data Analysis

```{r}
df_yes <- df %>%
  filter(y=="yes")
#summary(df_yes)

# Nothing interesting found in the below code so commenting it out
# ggplot(bank_additional_full, aes(x=age, y=emp.var.rate)) +
#   geom_point(size=1, shape="circle") +
#   ggtitle("Employment Variation Rate vs Age") + 
#   facet_wrap(~ y)

ggplot(df, aes(x=age, y=duration, color = y)) +
  geom_point(size=1, shape="circle") + 
  ggtitle("Duration vs Age")

ggplot(df, aes(x=age, y=cons_price_idx, color = y)) +
  geom_point(size=1, shape="circle") + 
  ggtitle("Consumer Price Index vs Age") 

# ggplot(bank_additional_full, aes(x=age, y=education)) +
#   geom_point(size=1, shape="circle") + 
#   ggtitle("Education vs Age")  + 
#    facet_wrap(~ y)
```

Duration vs Age: The duration of last contact (in seconds) was longer for ages 25-50. And it was understandably longer for "yes" vs for "no"

# Making new age group
```{r}
#Analysing Age
ggplot(df) + geom_histogram(mapping = aes(x=age, fill=y)) +ggtitle("Distribution of 'y' by age")
```

```{r}
#Creating new variables
#Age_Grp - split the data into age groups "17-31","32-37" ,"38-47", "47-55", ">55" (based in IQR)
df$Age_Grp <- cut(df$age, breaks = c(16,31,37,46,55,98), labels = c("17-31","32-37" ,"38-47", "47-55", ">55"))

#validate the cut command
#df %>% filter(!$Age_Grp  %in% c("17-31","32-37" ,"38-47", "47-55", ">55"))
#df %>% filter(df$age==55)

ggplot(df) + geom_bar(mapping = aes(x=Age_Grp, fill = y)) + ggtitle("Distribution of 'y' by Age_Grp") +
  ylab("Cnt") + xlab("Age Group")


```

We will keep both age and age group in our model to see if one is selected over the other. We need to make sure to not use both in our model building.

```{r}
#Analyzing pdays
ggplot(df) + geom_histogram(mapping = aes(x=pdays, fill=y))
```

- Analyzing 'pdays' ie., number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
- Most folks had no previous campaign but if they did, it looks like most who had a previous campaign decided to subscribe


```{r}
#zoom in for ones that were previously contacted
df %>%
  filter(pdays < 999) %>%
  ggplot() +
  geom_histogram(mapping = aes(x=pdays, fill=y))

```
Highest frequency appears to be less than 10 days since last contact. Let's make this into a y/n variable instead due to the large gap between days contacted and the '999' variable.


```{r}
#prevly_Cntctd Yes/No. TO see the distribution or 'Y' on first time contact vs. a follow up
df$prevly_Cntctd <- as.factor(case_when(df$pdays==999 ~ "No", !df$pdays==999 ~ "Yes"))

#Validate previously contacted variable
#df %>% filter(!df$pdays==999)

ggplot(df) + geom_bar(mapping = aes(x=prevly_Cntctd, fill = y)) + ggtitle("Number of 'y' by whether customers were prev.contacted or not") +
  ylab("Cnt") + xlab("Previously contacted?")

```

Same observation here as above: Most folks had no previous campaign but if they did, it looks like most who had a previous campaign decided to subscribe / likely to say "Yes". Since we have now created a new variable dependent on pdays, proceed to remove pdays to avoid issues with multicollinearity. Additionally, poutcome is dependent on whether or not someone has previously been contacted, and we have 'nonexistent' at 86% of the data. Remove this variable as well since it doesn't add much value and is dependent on pdays/previously contacted.

```{r}
df <- df %>% select(-pdays, -poutcome)
```

```{r}
#Analysing campaign
ggplot(df) + 
  geom_histogram(mapping = aes(x=campaign, fill=y)) +
  ggtitle("Distribution of 'y' by campaign")
```
- Just visually, when we decided to stop contacting a person it didn't affect our closing ratio which still dropped off precipitously 

- Ideally, the campaign would stop contacting people who are less likely to subscribe, and keep contacting people if they are more likely to subscribe.  Then we should see the ratio of Yes to No go up as the number of no contacts goes up.  Instead, it looks like the ratio stays the same and the number of Yes's drops proportionately with the number of No's. 

```{r}
#Analyzing job
ggplot(df) + 
  geom_bar(mapping = aes(x=job, fill = y)) + 
  coord_flip() +     #Added coord flip here to make it more readable
  ggtitle("Number of 'y' by job") +
  ylab("Count") + 
  xlab("Job")

```

"y" - has a client subscribed a term deposit? : admin, technician and blue collar jobs are the top 3 subscribers by volume 

```{r}
df2 <- df %>%
  group_by(job) %>%
  count(y) %>%
  mutate(job_conv = n/sum(n)) %>%
  filter(y == "yes")

ggplot(df2, aes(x=job, y=job_conv)) +
  geom_point() +
  coord_flip() 
```

Above, I looked at the ratio of "yes" vs "no" and see that students and retired persons convert at much higher rates than those of other professions. And 'blue collar' has the lowest conversion rate

So, if they were to want to improve the cost effectiveness of their campaigns they might want to target more 'students' and 'retirees'


```{r}
#Analyzing marital
ggplot(df) + 
  geom_bar(mapping = aes(x=marital, fill = y)) + 
  ggtitle("Number of 'y' by marital") +
  ylab("Cnt") + 
  xlab("marital")

```

- More 'married' people are represented in the campaign
- Visually looking, conversion rate seems to be higher for 'single' people

```{r}
#Analyzing duration and creating duration group variable
summary(df$duration)

df$duration_group <- 
  cut(df$duration, 
      breaks = c(-Inf,100,60,300,600,Inf), 
      labels = c("0-30s", "30-60s", "1-5 min", "5-10min", "10+ min"))

# Check for missing values
tibble(variable = names(colSums(is.na(df))),
       missing = colSums(is.na(df))) %>% 
  gt() %>% 
  tab_header(title = "Missing Values in Data")


df3 <- df %>%
  group_by(duration_group) %>%
  count(y) %>%
  mutate(duration_group_conv = n/sum(n)) %>%
  filter(y == "yes")

ggplot(df3, aes(x=duration_group, y=duration_group_conv)) +
  geom_point() +
  facet_wrap(~ y)
```

- Looking above, clearly conversion rate goes up the longer the most recent call

# Checking for correlation
```{r}
# Look for high correlation
df.numeric <- df[ , sapply(df, is.numeric)]

corrs = cor(df.numeric, use="everything") # Calculate correlations between all variables
high_corrs = findCorrelation(corrs, cutoff=abs(0.1))  # Find 'high' correlations among those variables (0.1 is not exactly "high" but there are so few numerics...)
corrs = cor(df.numeric[,high_corrs], use="everything") # get a data frame with only highly correlated variables

#Create corrplot for numeric variables
corrplot(corrs)

# pairs only on highly correlated variables... 
pairs(corrs,col=df$y)


```

Based on the correlation plot above, we see high correlation between 'euribor3m' and 'emp_var_rate' and to a lesser degree with 'nr_employed.' We also see 'nr_employed' and 'emp_var_rate' also highly correlated, which makes sense since you would expect the number of employees to vary at the same time as the employment variation rate. Let us proceed with removal of 'emp_var_rate' as that appears to be correlated at a higher rate to euribor3m.

```{r}
#remove emp_var_rate
df <- df %>% select(-emp_var_rate)
```


# Run random forest on down-sampled data set to check for variable importance 

I am running RF on a subset of the data to do a gross check for important variables and to determine if the new variables duration group and age group are deemed more important than the continuous variables of just raw duration and raw age.

```{r}
#move response variable to end of data set
df <- df %>% relocate(y, .after = last_col())

#randomly sample 10k obs
sample10k <- sample_n(df, 10000)

#down sample to balance response
set.seed(1)
downsample <- downSample(x = sample10k[, -20],
                         y = sample10k$y)
table(downsample$Class)


RFcontrol <- rfeControl(functions=rfFuncs, method="cv", number=5, verbose = FALSE)

set.seed(123)
subsets <- c(1:5, 10, 15, 20)
RFresults <- rfe(downsample[,1:19], downsample[[20]], sizes=subsets, rfeControl=RFcontrol)
RFresults
varImp(RFresults)
```
Based on this, remove 'duration_group' and 'age_group' as the continuous version of those variables had higher importance on the final model.

```{r}
df <- df %>% select(-duration_group, -Age_Grp)

```

# Run Initial Logistic Regression

```{r}
# Train/Test Split
set.seed(1234)
num_rows <- nrow(df)
train_idx <- sample(1:num_rows, 0.8 * num_rows)
test_idx <- setdiff(1:num_rows, train_idx)
train <- df[train_idx, ]
test <- df[test_idx, ]
```

```{r}
#set train control parameters
trControl <- trainControl(method = "cv", number = 10, classProbs = TRUE)

model1 <- train(y ~., data = train, method = "glm", family = "binomial", trControl = trControl)
model1
```


```{r}
summary(model1)
confusionMatrix(model1)
```



```{r}
#make predictions and create confusion matrix
pred_model1 <- predict(model1, newdata = test)

confusionMatrix(pred_model1,test$y)
```

# ROC Curve

```{r}

```


# Random Forest

Running Random Forest on the same pre-processed dataset as in the above glm

```{r}
cv_5 <- trainControl(method="cv", 
                        number = 5)

set.seed(1234)
rf_grid <- expand.grid(
  mtry = 2:8,
  splitrule = "gini",
  min.node.size = c(1)
  )

fitRF <- train(y ~ ., 
               data = train, 
               method = "ranger", 
               trControl = cv_5,
               num.threads = 4,
               tuneGrid=rf_grid)  

#preProcess = c("center","scale")

fitRF
plot(fitRF)
```

Optimal accuracy of 0.9157407 is happening at mtry=6


```{r}
confusionMatrix(fitRF)
```


```{r}
predRF <- predict(fitRF, newdata = test)
confusionMatrix(predRF, test$y)
```

On the test set, this model gets an Accuracy of 91.46% with sensitivity is over 96% while specificity is not that great at 48.77%

### WIP - alternative approaches

Here I'm experimenting with different approaches, including downsampling and allowing previously rejected variables. 

```{r}
#move response variable to end of data set
dfd <- full %>% relocate(y, .after = last_col())

dfd[sapply(dfd, is.character)] <- lapply(dfd[sapply(dfd, is.character)], 
                                       as.factor)

#down sample to balance response
set.seed(1)
downsample <- downSample(x = dfd[, -21],
                         y = dfd$y)
downsample <- downsample %>% rename(y = Class)
table(downsample$y)
```

The down sampling produced a perfectly balanced dataset for training. 


# Train/Test Split
```{r}
#dff <- downsample
#set.seed(1234)
#num_rows <- nrow(dff)
#train_idx <- sample(1:num_rows, 0.8 * num_rows)
#test_idx <- setdiff(1:num_rows, train_idx)
#train <- dff[train_idx, ]
#test <- dff[test_idx, ]
```


```{r}
fitRF <- train(y ~ ., 
               data = downsample, 
               method = "rf", 
               trControl = trainControl(method="cv", number = 5),
               preProcess = c("center","scale"))  
fitRF
```

Explanation: TBD

```{r}
confusionMatrix(fitRF)
```




```{r}
predRF <- predict(fitRF, newdata = dfd)
confusionMatrix(predRF, dfd$y)
```


